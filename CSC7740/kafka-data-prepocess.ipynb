{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6453063-352c-4ec1-a79a-764c89f90058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka connection successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from confluent_kafka import Producer, KafkaError\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Kafka configuration\n",
    "kafka_configuration = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # 確保 Kafka 服務運行於此地址\n",
    "    'client.id': 'weather_producer',\n",
    "    'queue.buffering.max.messages': 1000000,  # 增加緩衝的最大消息數量\n",
    "    'queue.buffering.max.kbytes': 1024000,  # 增加緩衝區的大小（1 GB）\n",
    "    'linger.ms': 100  # 讓 Producer 等待發送，批量處理\n",
    "}\n",
    "\n",
    "producer = Producer(kafka_configuration)\n",
    "\n",
    "# CSV 檔案路徑\n",
    "csv_file_path = '/Users/chive/documents/MacAir001STUDY/004Fall2024/CSC7740_BigData/CSC7740final/dataset/WeatherEvents_Jan2016-Dec2022.csv'\n",
    "\n",
    "# Kafka topic\n",
    "kafka_topic = 'weather'\n",
    "\n",
    "# 檢查 Kafka 是否能連線\n",
    "def check_kafka_connection():\n",
    "    try:\n",
    "        producer.list_topics(timeout=10)\n",
    "        print(\"Kafka connection successful.\")\n",
    "    except KafkaError as e:\n",
    "        if e.code() == KafkaError._TRANSPORT:\n",
    "            print(f\"ERROR! Kafka connection failed: {e}. Possible causes:\")\n",
    "            print(\"1. Kafka broker is not running on the specified address.\")\n",
    "            print(\"2. Incorrect broker address or port (check 'bootstrap.servers').\")\n",
    "            print(\"3. Firewall or network issues blocking the connection.\")\n",
    "        else:\n",
    "            print(f\"ERROR! Kafka connection failed: {e}\")\n",
    "\n",
    "# 訊息傳遞報告\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f'ERROR ! Message delivery has failed: {err}')\n",
    "    else:\n",
    "        print(f'Topic {msg.topic()} has received your message. Delivery Successful | Offset {msg.offset()}')\n",
    "\n",
    "# 檢查 CSV 檔案是否存在\n",
    "if not os.path.exists(csv_file_path):\n",
    "    print(f\"ERROR! File not found: {csv_file_path}\")\n",
    "else:\n",
    "    # 嘗試連線 Kafka\n",
    "    check_kafka_connection()\n",
    "\n",
    "    # 開啟 CSV 檔案並傳送訊息\n",
    "    with open(csv_file_path, 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "        header_row = next(csv_reader)  # 跳過表頭\n",
    "\n",
    "        for row in csv_reader:\n",
    "            message_value = ','.join(row)\n",
    "            try:\n",
    "                producer.produce(kafka_topic, value=message_value.encode('utf-8'), callback=delivery_report)\n",
    "            except BufferError as e:\n",
    "                print(f\"Local buffer is full, waiting for free space: {e}\")\n",
    "                producer.poll(1)  # 等待 Kafka 處理完消息釋放空間\n",
    "            \n",
    "            # 主動調用 poll() 來處理已發送的消息，避免緩衝區過滿\n",
    "            producer.poll(0)\n",
    "        \n",
    "        # 等待所有訊息傳送完成\n",
    "        print(\"Flushing messages...\")\n",
    "        producer.flush()  # 等待所有訊息完成傳送\n",
    "        print(\"All messages have been delivered or failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44bfcf7c-bcbb-43a1-8198-f5ad038527f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----+--------+-----------------+-----------+-----------+-----------+--------+-----+\n",
      "|     StartTime(UTC)|       EndTime(UTC)|Type|Severity|Precipitation(in)|AirportCode|LocationLat|LocationLng|    City|State|\n",
      "+-------------------+-------------------+----+--------+-----------------+-----------+-----------+-----------+--------+-----+\n",
      "|2016-01-06 23:14:00|2016-01-07 00:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-07 04:14:00|2016-01-07 04:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-07 05:54:00|2016-01-07 15:34:00|Snow|   Light|             0.03|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-08 05:34:00|2016-01-08 05:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-08 13:54:00|2016-01-08 15:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-08 16:14:00|2016-01-08 17:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 12:54:00|2016-01-09 15:34:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 15:34:00|2016-01-09 16:14:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 16:14:00|2016-01-09 16:34:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 16:34:00|2016-01-09 16:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 16:54:00|2016-01-09 20:34:00|Cold|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 02:54:00|2016-01-10 04:14:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 09:34:00|2016-01-10 10:14:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 10:14:00|2016-01-10 10:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 10:34:00|2016-01-10 10:54:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 11:14:00|2016-01-10 13:14:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 13:14:00|2016-01-10 21:34:00|Cold|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-11 12:14:00|2016-01-11 12:54:00|Cold|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-16 09:14:00|2016-01-16 09:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-16 10:34:00|2016-01-16 11:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "+-------------------+-------------------+----+--------+-----------------+-----------+-----------+-----------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "# Start a Spark session\n",
    "spark = SparkSession.builder.appName(\"WeatherDataProcessing\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka configuration\n",
    "kafka_bootstrap_servers = 'localhost:9092'\n",
    "kafka_topic = 'weather'\n",
    "\n",
    "# Define the schema for the incoming weather data\n",
    "weather_schema = StructType([\n",
    "    StructField('StartTime(UTC)', StringType(), True),\n",
    "    StructField('EndTime(UTC)', StringType(), True),\n",
    "    StructField('Type', StringType(), True),\n",
    "    StructField('Severity', StringType(), True),\n",
    "    StructField('Precipitation(in)', FloatType(), True),\n",
    "    StructField('AirportCode', StringType(), True),\n",
    "    StructField('LocationLat', StringType(), True),\n",
    "    StructField('LocationLng', StringType(), True),\n",
    "    StructField('City', StringType(), True),\n",
    "    StructField('State', StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the Kafka stream\n",
    "weather_data_df = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', kafka_bootstrap_servers) \\\n",
    "    .option('subscribe', kafka_topic) \\\n",
    "    .load()\n",
    "\n",
    "# Kafka value is in binary format, convert it to string\n",
    "weather_data_df = weather_data_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse the CSV data received from Kafka using the predefined schema\n",
    "weather_data_df = weather_data_df.select(from_json(col(\"value\"), weather_schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Convert StartTime and EndTime to proper timestamp formats\n",
    "weather_data_df = weather_data_df.withColumn(\"StartTime(UTC)\", to_timestamp(col(\"StartTime(UTC)\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"EndTime(UTC)\", to_timestamp(col(\"EndTime(UTC)\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Define relevant columns based on the new dataset\n",
    "DATETIME_START_COL = 'StartTime(UTC)'\n",
    "DATETIME_END_COL = 'EndTime(UTC)'\n",
    "EVENT_TYPE_COL = 'Type'\n",
    "SEVERITY_COL = 'Severity'\n",
    "PRECIPITATION_COL = 'Precipitation(in)'\n",
    "AIRPORTCODE_COL= 'AirportCode'\n",
    "LATITUDE_COL = 'LocationLat'\n",
    "LONGITUDE_COL = 'LocationLng'\n",
    "CITY_COL = 'City'\n",
    "STATE_COL = 'State'\n",
    "\n",
    "# Select the relevant columns to keep for further processing\n",
    "weather_data_selected_df = weather_data_df.select(DATETIME_START_COL, DATETIME_END_COL, EVENT_TYPE_COL, SEVERITY_COL, \n",
    "                                                  PRECIPITATION_COL, AIRPORTCODE_COL, LATITUDE_COL, LONGITUDE_COL, \n",
    "                                                  CITY_COL, STATE_COL)\n",
    "\n",
    "# Show the first few rows of the full dataset with selected columns\n",
    "weather_data_selected_df.show()\n",
    "\n",
    "# Start streaming and show the output\n",
    "query = weather_data_selected_df.writeStream \\\n",
    "    .outputMode('append') \\\n",
    "    .format('console') \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c473b1b0-dd8c-4c9e-bd10-f94be0d7ba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 8627181\n",
      "Number of columns: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the number of rows and columns in the filtered DataFrame\n",
    "num_rows = weather_data_selected_df.count()\n",
    "num_columns = len(weather_data_selected_df.columns)\n",
    "\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1966c3f7-282a-4cf2-84a4-c5a624b8ce0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StartTime(UTC): timestamp (nullable = true)\n",
      " |-- EndTime(UTC): timestamp (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Severity: string (nullable = true)\n",
      " |-- Precipitation(in): double (nullable = true)\n",
      " |-- AirportCode: string (nullable = true)\n",
      " |-- LocationLat: double (nullable = true)\n",
      " |-- LocationLng: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the entire dataset\n",
    "weather_data_selected_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49eb5698-0a1c-4fb1-9fac-0261c670157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `StartTime(UTC)` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `EndTime(UTC)` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `Type` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `Severity` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `Precipitation(in)` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `AirportCode` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `LocationLat` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `LocationLng` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `City` count: 16912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `State` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count missing values for each column in the full dataset\n",
    "for c in weather_data_selected_df.columns:\n",
    "    print(f'Missing values of column `{c}` count: {weather_data_df.where(col(c).isNull()).count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b371aeee-fa08-4378-9baf-5f352e89d5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----+--------+-----------------+-----------+-----------+-----------+--------+-----+\n",
      "|     StartTime(UTC)|       EndTime(UTC)|Type|Severity|Precipitation(in)|AirportCode|LocationLat|LocationLng|    City|State|\n",
      "+-------------------+-------------------+----+--------+-----------------+-----------+-----------+-----------+--------+-----+\n",
      "|2016-01-06 23:14:00|2016-01-07 00:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-07 04:14:00|2016-01-07 04:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-07 05:54:00|2016-01-07 15:34:00|Snow|   Light|             0.03|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-08 05:34:00|2016-01-08 05:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-08 13:54:00|2016-01-08 15:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-08 16:14:00|2016-01-08 17:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 12:54:00|2016-01-09 15:34:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 15:34:00|2016-01-09 16:14:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 16:14:00|2016-01-09 16:34:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 16:34:00|2016-01-09 16:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 16:54:00|2016-01-09 20:34:00|Cold|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 02:54:00|2016-01-10 04:14:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 09:34:00|2016-01-10 10:14:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 10:14:00|2016-01-10 10:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 10:34:00|2016-01-10 10:54:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 11:14:00|2016-01-10 13:14:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 13:14:00|2016-01-10 21:34:00|Cold|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-11 12:14:00|2016-01-11 12:54:00|Cold|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-16 09:14:00|2016-01-16 09:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-16 10:34:00|2016-01-16 11:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "+-------------------+-------------------+----+--------+-----------------+-----------+-----------+-----------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with any null values in the full dataset\n",
    "not_null_weather_data_df = weather_data_selected_df.dropna()\n",
    "\n",
    "# Show the result after dropping rows with null values\n",
    "not_null_weather_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81cd99a5-1148-4034-80eb-cefc940cc296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 104:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|Type         |count  |\n",
      "+-------------+-------+\n",
      "|Cold         |231232 |\n",
      "|Fog          |2009035|\n",
      "|Storm        |61096  |\n",
      "|Precipitation|157036 |\n",
      "|Hail         |2921   |\n",
      "|Snow         |1156334|\n",
      "|Rain         |4992615|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Specific weather columns: Group by 'Type' column and count occurrences in the filtered dataset\n",
    "not_null_weather_data_df.groupBy(EVENT_TYPE_COL).count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27e4b150-572a-4a63-9be9-d819cbf4525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Iterable\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Weather condition dictionary to map specific data points to broader categories\n",
    "def get_weather_conditions_aggregation_dict(weather_conditions: Iterable[str]) -> Dict[str, str]:\n",
    "    \n",
    "    weather_conditions_dict = dict()\n",
    "  \n",
    "    for weather_condition in weather_conditions:\n",
    "        weather_condition_lowered = weather_condition.lower()\n",
    "\n",
    "        if any(key in weather_condition_lowered for key in ['squall', 'thunderstorm']):\n",
    "            weather_conditions_dict[weather_condition] = 'thunderstorm'\n",
    "        elif any(key in weather_condition_lowered for key in ['drizzle', 'rain']):\n",
    "            weather_conditions_dict[weather_condition] = 'rainy'\n",
    "        elif any(key in weather_condition_lowered for key in ['sleet', 'snow']):\n",
    "            weather_conditions_dict[weather_condition] = 'snowy'\n",
    "        elif 'cloud' in weather_condition_lowered:\n",
    "            weather_conditions_dict[weather_condition] = 'cloudy'\n",
    "        elif any(key in weather_condition_lowered for key in ['fog', 'mist', 'haze']):\n",
    "            weather_conditions_dict[weather_condition] = 'foggy'\n",
    "        elif any(key in weather_condition_lowered for key in ['clear', 'sun']):\n",
    "            weather_conditions_dict[weather_condition] = 'sunny'\n",
    "        elif 'cold' in weather_condition_lowered:\n",
    "            weather_conditions_dict[weather_condition] = 'cold'\n",
    "        elif 'hail' in weather_condition_lowered:\n",
    "            weather_conditions_dict[weather_condition] = 'hail'\n",
    "    \n",
    "    return weather_conditions_dict\n",
    "\n",
    "# Select distinct weather conditions (event types) from the dataset\n",
    "weather_conditions_all = not_null_weather_data_df.select(EVENT_TYPE_COL).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Create a dictionary that maps the weather event types to broader categories\n",
    "weather_conditions_dict = get_weather_conditions_aggregation_dict(weather_conditions_all)\n",
    "\n",
    "# Broadcast the dictionary for efficient lookup in a distributed environment\n",
    "broadcast_dict = spark.sparkContext.broadcast(weather_conditions_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "320cbf2a-4f69-459e-a7cd-f676d212b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# UDF to map weather event types to broad categories\n",
    "def map_weather_condition(weather_condition):\n",
    "    return broadcast_dict.value.get(weather_condition, \"Unknown\")\n",
    "\n",
    "# Register the UDF to map specific weather conditions\n",
    "map_weather_condition_udf = udf(map_weather_condition, StringType())\n",
    "\n",
    "# Apply the UDF to the event type column to create a new column with broad weather conditions\n",
    "weather_measurements_aggregate_df = not_null_weather_data_df.withColumn(\n",
    "    \"Broad_Weather_Condition\",\n",
    "    map_weather_condition_udf(col(EVENT_TYPE_COL))\n",
    ")\n",
    "\n",
    "# Optionally, drop the original specific weather event type column\n",
    "weather_measurements_aggregate_general_df = weather_measurements_aggregate_df.drop(EVENT_TYPE_COL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01c4db5d-06df-402d-9bd4-d59c5dad4abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+-----------------+-----------+-----------+-----------+--------+-----+-----------------------+\n",
      "|     StartTime(UTC)|       EndTime(UTC)|Severity|Precipitation(in)|AirportCode|LocationLat|LocationLng|    City|State|Broad_Weather_Condition|\n",
      "+-------------------+-------------------+--------+-----------------+-----------+-----------+-----------+--------+-----+-----------------------+\n",
      "|2016-01-06 23:14:00|2016-01-07 00:34:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-07 04:14:00|2016-01-07 04:54:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-07 05:54:00|2016-01-07 15:34:00|   Light|             0.03|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-08 05:34:00|2016-01-08 05:54:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-08 13:54:00|2016-01-08 15:54:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-08 16:14:00|2016-01-08 17:34:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-09 12:54:00|2016-01-09 15:34:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  foggy|\n",
      "|2016-01-09 15:34:00|2016-01-09 16:14:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-09 16:14:00|2016-01-09 16:34:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  foggy|\n",
      "|2016-01-09 16:34:00|2016-01-09 16:54:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-09 16:54:00|2016-01-09 20:34:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                   cold|\n",
      "|2016-01-10 02:54:00|2016-01-10 04:14:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  foggy|\n",
      "|2016-01-10 09:34:00|2016-01-10 10:14:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  foggy|\n",
      "|2016-01-10 10:14:00|2016-01-10 10:34:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-10 10:34:00|2016-01-10 10:54:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  foggy|\n",
      "|2016-01-10 11:14:00|2016-01-10 13:14:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  foggy|\n",
      "|2016-01-10 13:14:00|2016-01-10 21:34:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                   cold|\n",
      "|2016-01-11 12:14:00|2016-01-11 12:54:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                   cold|\n",
      "|2016-01-16 09:14:00|2016-01-16 09:54:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-16 10:34:00|2016-01-16 11:34:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "+-------------------+-------------------+--------+-----------------+-----------+-----------+-----------+--------+-----+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Not null and consolidated dataframe with broad weather conditions\n",
    "SLOW_OPERATIONS: bool = True\n",
    "if SLOW_OPERATIONS: \n",
    "    weather_measurements_aggregate_general_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd326096-617d-45fb-a0f6-be5f40dc0a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------+\n",
      "|Broad_Weather_Condition|count  |\n",
      "+-----------------------+-------+\n",
      "|rainy                  |4992615|\n",
      "|snowy                  |1156334|\n",
      "|Unknown                |218132 |\n",
      "|cold                   |231232 |\n",
      "|hail                   |2921   |\n",
      "|foggy                  |2009035|\n",
      "+-----------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the broad weather condition column\n",
    "BROAD_WEATHER_COL = 'Broad_Weather_Condition'\n",
    "\n",
    "# Group by the broad weather condition column and count the occurrences\n",
    "weather_measurements_aggregate_general_df.groupBy(BROAD_WEATHER_COL).count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab9d2449-4a32-4e84-afa9-3284907a5c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the broad weather condition column name\n",
    "BROAD_WEATHER_COL = 'Broad_Weather_Condition'\n",
    "\n",
    "# Filter out the rows where Broad_Weather_Condition is 'Unknown'\n",
    "weather_aggregate_filtered_df = weather_measurements_aggregate_general_df.filter(col(BROAD_WEATHER_COL) != \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "896700a4-f731-4235-bbff-04701983e371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 118:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----+\n",
      "|Broad_Weather_Condition|count|\n",
      "+-----------------------+-----+\n",
      "|rainy                  |10000|\n",
      "|snowy                  |10000|\n",
      "|cold                   |10000|\n",
      "|hail                   |2921 |\n",
      "|foggy                  |10000|\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "# Define the window specification, ordering by start time in descending order within each broad weather condition\n",
    "windowSpec = Window.partitionBy(BROAD_WEATHER_COL).orderBy(col(DATETIME_START_COL).desc())\n",
    "\n",
    "# Add row number to each row within the window\n",
    "weather_ranked_df = weather_aggregate_filtered_df.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "\n",
    "# Filter the ranked dataframe to get the latest 10,000 datasets for each Broad_Weather_Condition\n",
    "weather_aggregate_10k_df = weather_ranked_df.filter(col(\"row_number\") <= 10000)\n",
    "\n",
    "# Drop the row_number column\n",
    "weather_aggregate_final_df = weather_aggregate_10k_df.drop(\"row_number\")\n",
    "\n",
    "# Show the final dataframe, grouped by Broad_Weather_Condition and showing the count for each category\n",
    "weather_aggregate_final_df.groupBy(BROAD_WEATHER_COL).count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ca58366-e81e-4a03-acf2-fef03e928378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Statistical distribution of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c5b36ef-98cd-4eb5-81cb-d0ff6e633b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-----------+-----------------+------------------+----------+-----+-----------------------+\n",
      "|summary|Severity|  Precipitation(in)|AirportCode|      LocationLat|       LocationLng|      City|State|Broad_Weather_Condition|\n",
      "+-------+--------+-------------------+-----------+-----------------+------------------+----------+-----+-----------------------+\n",
      "|  count|   42921|              42921|      42921|            42921|             42921|     42921|42921|                  42921|\n",
      "|   mean|    NULL|0.02936278278698031|       NULL|39.98051975955862|-96.75347946459746|      NULL| NULL|                   NULL|\n",
      "| stddev|    NULL|0.12242399472858015|       NULL| 5.36094191369729|15.394954701008139|      NULL| NULL|                   NULL|\n",
      "|    min|   Heavy|                0.0|       K01M|          24.5571|          -124.555| Abbeville|   AL|                   cold|\n",
      "|    max|  Severe|               4.87|       KZZV|          48.9402|          -67.7928|Zionsville|   WY|                  snowy|\n",
      "+-------+--------+-------------------+-----------+-----------------+------------------+----------+-----+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display summary statistics of the numerical columns in the final dataframe\n",
    "weather_aggregate_final_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23df53bf-8c6c-4454-b66c-ff4512f0d962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Path where the CSV file will be saved\n",
    "final_dataset_path = \"dataset/final_dataset\"\n",
    "\n",
    "# Save the DataFrame as a single CSV file\n",
    "(weather_aggregate_final_df\n",
    "    .coalesce(1)  # Combine all partitions into one\n",
    "    .write\n",
    "    .mode(\"overwrite\")  # Overwrite if the file already exists\n",
    "    .option(\"header\", \"true\")  # Include header\n",
    "    .csv(final_dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529bda5-34a8-4815-940f-564ff15b1842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
