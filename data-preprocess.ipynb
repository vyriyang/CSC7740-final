{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0cd0d8-206d-4ad6-9fbd-e8a923f88617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chive/documents/MacAir001STUDY/004Fall2024/CSC7740_BigData/CSC7740final/dataset/.DS_Store\n",
      "/Users/chive/documents/MacAir001STUDY/004Fall2024/CSC7740_BigData/CSC7740final/dataset/WeatherEvents_Jan2016-Dec2022.csv\n",
      "/Users/chive/documents/MacAir001STUDY/004Fall2024/CSC7740_BigData/CSC7740final/dataset/weather_events_demo.csv\n",
      "/Users/chive/documents/MacAir001STUDY/004Fall2024/CSC7740_BigData/CSC7740final/dataset/final_dataset/.part-00000-e6e632a8-4d8f-4580-8fda-e4d46dd12652-c000.csv.crc\n",
      "/Users/chive/documents/MacAir001STUDY/004Fall2024/CSC7740_BigData/CSC7740final/dataset/final_dataset/part-00000-e6e632a8-4d8f-4580-8fda-e4d46dd12652-c000.csv\n",
      "/Users/chive/documents/MacAir001STUDY/004Fall2024/CSC7740_BigData/CSC7740final/dataset/final_dataset/._SUCCESS.crc\n",
      "/Users/chive/documents/MacAir001STUDY/004Fall2024/CSC7740_BigData/CSC7740final/dataset/final_dataset/_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import folium # plotting library\n",
    "from folium import plugins\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/Users/chive/documents/MacAir001STUDY/004Fall2024/CSC7740_BigData/CSC7740final/dataset'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753b989a-11cb-4d1d-9182-9d66964173cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>Type</th>\n",
       "      <th>Severity</th>\n",
       "      <th>StartTime(UTC)</th>\n",
       "      <th>EndTime(UTC)</th>\n",
       "      <th>Precipitation(in)</th>\n",
       "      <th>TimeZone</th>\n",
       "      <th>AirportCode</th>\n",
       "      <th>LocationLat</th>\n",
       "      <th>LocationLng</th>\n",
       "      <th>City</th>\n",
       "      <th>County</th>\n",
       "      <th>State</th>\n",
       "      <th>ZipCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W-1</td>\n",
       "      <td>Snow</td>\n",
       "      <td>Light</td>\n",
       "      <td>2016-01-06 23:14:00</td>\n",
       "      <td>2016-01-07 00:34:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>US/Mountain</td>\n",
       "      <td>K04V</td>\n",
       "      <td>38.0972</td>\n",
       "      <td>-106.1689</td>\n",
       "      <td>Saguache</td>\n",
       "      <td>Saguache</td>\n",
       "      <td>CO</td>\n",
       "      <td>81149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W-2</td>\n",
       "      <td>Snow</td>\n",
       "      <td>Light</td>\n",
       "      <td>2016-01-07 04:14:00</td>\n",
       "      <td>2016-01-07 04:54:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>US/Mountain</td>\n",
       "      <td>K04V</td>\n",
       "      <td>38.0972</td>\n",
       "      <td>-106.1689</td>\n",
       "      <td>Saguache</td>\n",
       "      <td>Saguache</td>\n",
       "      <td>CO</td>\n",
       "      <td>81149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W-3</td>\n",
       "      <td>Snow</td>\n",
       "      <td>Light</td>\n",
       "      <td>2016-01-07 05:54:00</td>\n",
       "      <td>2016-01-07 15:34:00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>US/Mountain</td>\n",
       "      <td>K04V</td>\n",
       "      <td>38.0972</td>\n",
       "      <td>-106.1689</td>\n",
       "      <td>Saguache</td>\n",
       "      <td>Saguache</td>\n",
       "      <td>CO</td>\n",
       "      <td>81149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W-4</td>\n",
       "      <td>Snow</td>\n",
       "      <td>Light</td>\n",
       "      <td>2016-01-08 05:34:00</td>\n",
       "      <td>2016-01-08 05:54:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>US/Mountain</td>\n",
       "      <td>K04V</td>\n",
       "      <td>38.0972</td>\n",
       "      <td>-106.1689</td>\n",
       "      <td>Saguache</td>\n",
       "      <td>Saguache</td>\n",
       "      <td>CO</td>\n",
       "      <td>81149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W-5</td>\n",
       "      <td>Snow</td>\n",
       "      <td>Light</td>\n",
       "      <td>2016-01-08 13:54:00</td>\n",
       "      <td>2016-01-08 15:54:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>US/Mountain</td>\n",
       "      <td>K04V</td>\n",
       "      <td>38.0972</td>\n",
       "      <td>-106.1689</td>\n",
       "      <td>Saguache</td>\n",
       "      <td>Saguache</td>\n",
       "      <td>CO</td>\n",
       "      <td>81149.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  EventId  Type Severity       StartTime(UTC)         EndTime(UTC)  \\\n",
       "0     W-1  Snow    Light  2016-01-06 23:14:00  2016-01-07 00:34:00   \n",
       "1     W-2  Snow    Light  2016-01-07 04:14:00  2016-01-07 04:54:00   \n",
       "2     W-3  Snow    Light  2016-01-07 05:54:00  2016-01-07 15:34:00   \n",
       "3     W-4  Snow    Light  2016-01-08 05:34:00  2016-01-08 05:54:00   \n",
       "4     W-5  Snow    Light  2016-01-08 13:54:00  2016-01-08 15:54:00   \n",
       "\n",
       "   Precipitation(in)     TimeZone AirportCode  LocationLat  LocationLng  \\\n",
       "0               0.00  US/Mountain        K04V      38.0972    -106.1689   \n",
       "1               0.00  US/Mountain        K04V      38.0972    -106.1689   \n",
       "2               0.03  US/Mountain        K04V      38.0972    -106.1689   \n",
       "3               0.00  US/Mountain        K04V      38.0972    -106.1689   \n",
       "4               0.00  US/Mountain        K04V      38.0972    -106.1689   \n",
       "\n",
       "       City    County State  ZipCode  \n",
       "0  Saguache  Saguache    CO  81149.0  \n",
       "1  Saguache  Saguache    CO  81149.0  \n",
       "2  Saguache  Saguache    CO  81149.0  \n",
       "3  Saguache  Saguache    CO  81149.0  \n",
       "4  Saguache  Saguache    CO  81149.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/chive/documents/MacAir001STUDY/004Fall2024/CSC7740_BigData/CSC7740final/dataset/WeatherEvents_Jan2016-Dec2022.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee33e5b7-d69c-494b-95fd-6b76eba4fc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/26 22:09:02 WARN Utils: Your hostname, CHIVEs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.44 instead (on interface en0)\n",
      "24/09/26 22:09:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/26 22:09:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----+--------+-----------------+-----------+-----------+-----------+--------+-----+\n",
      "|     StartTime(UTC)|       EndTime(UTC)|Type|Severity|Precipitation(in)|AirportCode|LocationLat|LocationLng|    City|State|\n",
      "+-------------------+-------------------+----+--------+-----------------+-----------+-----------+-----------+--------+-----+\n",
      "|2016-01-06 23:14:00|2016-01-07 00:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-07 04:14:00|2016-01-07 04:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-07 05:54:00|2016-01-07 15:34:00|Snow|   Light|             0.03|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-08 05:34:00|2016-01-08 05:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-08 13:54:00|2016-01-08 15:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-08 16:14:00|2016-01-08 17:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 12:54:00|2016-01-09 15:34:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 15:34:00|2016-01-09 16:14:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 16:14:00|2016-01-09 16:34:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 16:34:00|2016-01-09 16:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 16:54:00|2016-01-09 20:34:00|Cold|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 02:54:00|2016-01-10 04:14:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 09:34:00|2016-01-10 10:14:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 10:14:00|2016-01-10 10:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 10:34:00|2016-01-10 10:54:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 11:14:00|2016-01-10 13:14:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 13:14:00|2016-01-10 21:34:00|Cold|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-11 12:14:00|2016-01-11 12:54:00|Cold|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-16 09:14:00|2016-01-16 09:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-16 10:34:00|2016-01-16 11:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "+-------------------+-------------------+----+--------+-----------------+-----------+-----------+-----------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Start a Spark session\n",
    "spark = SparkSession.builder.appName(\"WeatherDataProcessing\").config(\"spark.driver.memory\", \"5g\").getOrCreate()\n",
    "\n",
    "# Path to the new dataset\n",
    "weather_data_path = '/Users/chive/documents/MacAir001STUDY/004Fall2024/CSC7740_BigData/CSC7740final/dataset/WeatherEvents_Jan2016-Dec2022.csv'  # Update this with the correct path on your system\n",
    "\n",
    "# Read CSV file into Spark DataFrame\n",
    "weather_data_df = spark.read.csv(weather_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Define relevant columns based on the new dataset\n",
    "DATETIME_START_COL = 'StartTime(UTC)'\n",
    "DATETIME_END_COL = 'EndTime(UTC)'\n",
    "EVENT_TYPE_COL = 'Type'\n",
    "SEVERITY_COL = 'Severity'\n",
    "PRECIPITATION_COL = 'Precipitation(in)'\n",
    "AIRPORTCODE_COL= 'AirportCode'\n",
    "LATITUDE_COL = 'LocationLat'\n",
    "LONGITUDE_COL = 'LocationLng'\n",
    "CITY_COL = 'City'\n",
    "STATE_COL = 'State'\n",
    "\n",
    "# Select the relevant columns to keep for further processing\n",
    "weather_data_selected_df = weather_data_df.select(DATETIME_START_COL, DATETIME_END_COL, EVENT_TYPE_COL, SEVERITY_COL, \n",
    "                                                  PRECIPITATION_COL, AIRPORTCODE_COL, LATITUDE_COL, LONGITUDE_COL, CITY_COL, STATE_COL)\n",
    "\n",
    "# Show the first few rows of the full dataset with selected columns\n",
    "weather_data_selected_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff35d67b-f7a5-4eaf-ad3f-8a736c407dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 8627181\n",
      "Number of columns: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the number of rows and columns in the filtered DataFrame\n",
    "num_rows = weather_data_selected_df.count()\n",
    "num_columns = len(weather_data_selected_df.columns)\n",
    "\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cab52fb3-ec45-44be-b944-6748c231e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StartTime(UTC): timestamp (nullable = true)\n",
      " |-- EndTime(UTC): timestamp (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Severity: string (nullable = true)\n",
      " |-- Precipitation(in): double (nullable = true)\n",
      " |-- AirportCode: string (nullable = true)\n",
      " |-- LocationLat: double (nullable = true)\n",
      " |-- LocationLng: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the entire dataset\n",
    "weather_data_selected_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6636527-ffbe-468e-a0fb-58de4e213e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `StartTime(UTC)` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `EndTime(UTC)` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `Type` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `Severity` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `Precipitation(in)` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `AirportCode` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `LocationLat` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `LocationLng` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `City` count: 16912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `State` count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count missing values for each column in the full dataset\n",
    "for c in weather_data_selected_df.columns:\n",
    "    print(f'Missing values of column `{c}` count: {weather_data_df.where(col(c).isNull()).count()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c86416a-65e0-4f2d-99e2-d1d282a7d972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----+--------+-----------------+-----------+-----------+-----------+--------+-----+\n",
      "|     StartTime(UTC)|       EndTime(UTC)|Type|Severity|Precipitation(in)|AirportCode|LocationLat|LocationLng|    City|State|\n",
      "+-------------------+-------------------+----+--------+-----------------+-----------+-----------+-----------+--------+-----+\n",
      "|2016-01-06 23:14:00|2016-01-07 00:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-07 04:14:00|2016-01-07 04:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-07 05:54:00|2016-01-07 15:34:00|Snow|   Light|             0.03|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-08 05:34:00|2016-01-08 05:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-08 13:54:00|2016-01-08 15:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-08 16:14:00|2016-01-08 17:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 12:54:00|2016-01-09 15:34:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 15:34:00|2016-01-09 16:14:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 16:14:00|2016-01-09 16:34:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 16:34:00|2016-01-09 16:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-09 16:54:00|2016-01-09 20:34:00|Cold|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 02:54:00|2016-01-10 04:14:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 09:34:00|2016-01-10 10:14:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 10:14:00|2016-01-10 10:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 10:34:00|2016-01-10 10:54:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 11:14:00|2016-01-10 13:14:00| Fog|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-10 13:14:00|2016-01-10 21:34:00|Cold|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-11 12:14:00|2016-01-11 12:54:00|Cold|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-16 09:14:00|2016-01-16 09:54:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "|2016-01-16 10:34:00|2016-01-16 11:34:00|Snow|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|\n",
      "+-------------------+-------------------+----+--------+-----------------+-----------+-----------+-----------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with any null values in the full dataset\n",
    "not_null_weather_data_df = weather_data_selected_df.dropna()\n",
    "\n",
    "# Show the result after dropping rows with null values\n",
    "not_null_weather_data_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "796b7725-b8fc-4fe9-ae66-9415c584c614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|Type         |count  |\n",
      "+-------------+-------+\n",
      "|Cold         |231232 |\n",
      "|Fog          |2009035|\n",
      "|Storm        |61096  |\n",
      "|Precipitation|157036 |\n",
      "|Hail         |2921   |\n",
      "|Snow         |1156334|\n",
      "|Rain         |4992615|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Specific weather columns: Group by 'Type' column and count occurrences in the filtered dataset\n",
    "not_null_weather_data_df.groupBy(EVENT_TYPE_COL).count().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccaf367d-fbc8-47c5-9015-ce32a660c4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Iterable\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Weather condition dictionary to map specific data points to broader categories\n",
    "def get_weather_conditions_aggregation_dict(weather_conditions: Iterable[str]) -> Dict[str, str]:\n",
    "    \n",
    "    weather_conditions_dict = dict()\n",
    "  \n",
    "    for weather_condition in weather_conditions:\n",
    "        weather_condition_lowered = weather_condition.lower()\n",
    "\n",
    "        if any(key in weather_condition_lowered for key in ['squall', 'thunderstorm']):\n",
    "            weather_conditions_dict[weather_condition] = 'thunderstorm'\n",
    "        elif any(key in weather_condition_lowered for key in ['drizzle', 'rain']):\n",
    "            weather_conditions_dict[weather_condition] = 'rainy'\n",
    "        elif any(key in weather_condition_lowered for key in ['sleet', 'snow']):\n",
    "            weather_conditions_dict[weather_condition] = 'snowy'\n",
    "        elif 'cloud' in weather_condition_lowered:\n",
    "            weather_conditions_dict[weather_condition] = 'cloudy'\n",
    "        elif any(key in weather_condition_lowered for key in ['fog', 'mist', 'haze']):\n",
    "            weather_conditions_dict[weather_condition] = 'foggy'\n",
    "        elif any(key in weather_condition_lowered for key in ['clear', 'sun']):\n",
    "            weather_conditions_dict[weather_condition] = 'sunny'\n",
    "        elif 'cold' in weather_condition_lowered:\n",
    "            weather_conditions_dict[weather_condition] = 'cold'\n",
    "        elif 'hail' in weather_condition_lowered:\n",
    "            weather_conditions_dict[weather_condition] = 'hail'\n",
    "    \n",
    "    return weather_conditions_dict\n",
    "\n",
    "# Select distinct weather conditions (event types) from the dataset\n",
    "weather_conditions_all = not_null_weather_data_df.select(EVENT_TYPE_COL).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Create a dictionary that maps the weather event types to broader categories\n",
    "weather_conditions_dict = get_weather_conditions_aggregation_dict(weather_conditions_all)\n",
    "\n",
    "# Broadcast the dictionary for efficient lookup in a distributed environment\n",
    "broadcast_dict = spark.sparkContext.broadcast(weather_conditions_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a17eb202-5dd8-44fc-8ab6-db1b78fe3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# UDF to map weather event types to broad categories\n",
    "def map_weather_condition(weather_condition):\n",
    "    return broadcast_dict.value.get(weather_condition, \"Unknown\")\n",
    "\n",
    "# Register the UDF to map specific weather conditions\n",
    "map_weather_condition_udf = udf(map_weather_condition, StringType())\n",
    "\n",
    "# Apply the UDF to the event type column to create a new column with broad weather conditions\n",
    "weather_measurements_aggregate_df = not_null_weather_data_df.withColumn(\n",
    "    \"Broad_Weather_Condition\",\n",
    "    map_weather_condition_udf(col(EVENT_TYPE_COL))\n",
    ")\n",
    "\n",
    "# Optionally, drop the original specific weather event type column\n",
    "weather_measurements_aggregate_general_df = weather_measurements_aggregate_df.drop(EVENT_TYPE_COL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73f87eb7-e550-4638-b65a-4a96b7936bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+-----------------+-----------+-----------+-----------+--------+-----+-----------------------+\n",
      "|     StartTime(UTC)|       EndTime(UTC)|Severity|Precipitation(in)|AirportCode|LocationLat|LocationLng|    City|State|Broad_Weather_Condition|\n",
      "+-------------------+-------------------+--------+-----------------+-----------+-----------+-----------+--------+-----+-----------------------+\n",
      "|2016-01-06 23:14:00|2016-01-07 00:34:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-07 04:14:00|2016-01-07 04:54:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-07 05:54:00|2016-01-07 15:34:00|   Light|             0.03|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-08 05:34:00|2016-01-08 05:54:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-08 13:54:00|2016-01-08 15:54:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-08 16:14:00|2016-01-08 17:34:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-09 12:54:00|2016-01-09 15:34:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  foggy|\n",
      "|2016-01-09 15:34:00|2016-01-09 16:14:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-09 16:14:00|2016-01-09 16:34:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  foggy|\n",
      "|2016-01-09 16:34:00|2016-01-09 16:54:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-09 16:54:00|2016-01-09 20:34:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                   cold|\n",
      "|2016-01-10 02:54:00|2016-01-10 04:14:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  foggy|\n",
      "|2016-01-10 09:34:00|2016-01-10 10:14:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  foggy|\n",
      "|2016-01-10 10:14:00|2016-01-10 10:34:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-10 10:34:00|2016-01-10 10:54:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  foggy|\n",
      "|2016-01-10 11:14:00|2016-01-10 13:14:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  foggy|\n",
      "|2016-01-10 13:14:00|2016-01-10 21:34:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                   cold|\n",
      "|2016-01-11 12:14:00|2016-01-11 12:54:00|  Severe|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                   cold|\n",
      "|2016-01-16 09:14:00|2016-01-16 09:54:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "|2016-01-16 10:34:00|2016-01-16 11:34:00|   Light|              0.0|       K04V|    38.0972|  -106.1689|Saguache|   CO|                  snowy|\n",
      "+-------------------+-------------------+--------+-----------------+-----------+-----------+-----------+--------+-----+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Not null and consolidated dataframe with broad weather conditions\n",
    "SLOW_OPERATIONS: bool = True\n",
    "if SLOW_OPERATIONS: \n",
    "    weather_measurements_aggregate_general_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c74fa67-d1c4-48b3-9a47-a608724214fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------+\n",
      "|Broad_Weather_Condition|count  |\n",
      "+-----------------------+-------+\n",
      "|rainy                  |4992615|\n",
      "|snowy                  |1156334|\n",
      "|Unknown                |218132 |\n",
      "|cold                   |231232 |\n",
      "|hail                   |2921   |\n",
      "|foggy                  |2009035|\n",
      "+-----------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the broad weather condition column\n",
    "BROAD_WEATHER_COL = 'Broad_Weather_Condition'\n",
    "\n",
    "# Group by the broad weather condition column and count the occurrences\n",
    "weather_measurements_aggregate_general_df.groupBy(BROAD_WEATHER_COL).count().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2afd9ce1-6e50-4fa2-8b2e-17437617fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the broad weather condition column name\n",
    "BROAD_WEATHER_COL = 'Broad_Weather_Condition'\n",
    "\n",
    "# Filter out the rows where Broad_Weather_Condition is 'Unknown'\n",
    "weather_aggregate_filtered_df = weather_measurements_aggregate_general_df.filter(col(BROAD_WEATHER_COL) != \"Unknown\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c31e140e-9c6a-4751-9ead-6db84e5a024d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----+\n",
      "|Broad_Weather_Condition|count|\n",
      "+-----------------------+-----+\n",
      "|rainy                  |10000|\n",
      "|snowy                  |10000|\n",
      "|cold                   |10000|\n",
      "|hail                   |2921 |\n",
      "|foggy                  |10000|\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "# Define the window specification, ordering by start time in descending order within each broad weather condition\n",
    "windowSpec = Window.partitionBy(BROAD_WEATHER_COL).orderBy(col(DATETIME_START_COL).desc())\n",
    "\n",
    "# Add row number to each row within the window\n",
    "weather_ranked_df = weather_aggregate_filtered_df.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "\n",
    "# Filter the ranked dataframe to get the latest 10,000 datasets for each Broad_Weather_Condition\n",
    "weather_aggregate_10k_df = weather_ranked_df.filter(col(\"row_number\") <= 10000)\n",
    "\n",
    "# Drop the row_number column\n",
    "weather_aggregate_final_df = weather_aggregate_10k_df.drop(\"row_number\")\n",
    "\n",
    "# Show the final dataframe, grouped by Broad_Weather_Condition and showing the count for each category\n",
    "weather_aggregate_final_df.groupBy(BROAD_WEATHER_COL).count().show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dfc55c9-2586-4ba8-8b19-7171c8bb9126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Statistical distribution of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4ebea45-900d-4d61-ac66-22ee2fd955b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/26 22:13:11 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 57:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-----------+-----------------+------------------+----------+-----+-----------------------+\n",
      "|summary|Severity|  Precipitation(in)|AirportCode|      LocationLat|       LocationLng|      City|State|Broad_Weather_Condition|\n",
      "+-------+--------+-------------------+-----------+-----------------+------------------+----------+-----+-----------------------+\n",
      "|  count|   42921|              42921|      42921|            42921|             42921|     42921|42921|                  42921|\n",
      "|   mean|    NULL|0.02936278278698031|       NULL|39.98051975955862|-96.75347946459746|      NULL| NULL|                   NULL|\n",
      "| stddev|    NULL|0.12242399472858015|       NULL| 5.36094191369729|15.394954701008139|      NULL| NULL|                   NULL|\n",
      "|    min|   Heavy|                0.0|       K01M|          24.5571|          -124.555| Abbeville|   AL|                   cold|\n",
      "|    max|  Severe|               4.87|       KZZV|          48.9402|          -67.7928|Zionsville|   WY|                  snowy|\n",
      "+-------+--------+-------------------+-----------+-----------------+------------------+----------+-----+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display summary statistics of the numerical columns in the final dataframe\n",
    "weather_aggregate_final_df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "568e41f9-a68e-4455-b14c-323df54e70ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Path where the CSV file will be saved\n",
    "final_dataset_path = \"dataset/updated_final_dataset\"\n",
    "\n",
    "# Save the DataFrame as a single CSV file\n",
    "(weather_aggregate_final_df\n",
    "    .coalesce(1)  # Combine all partitions into one\n",
    "    .write\n",
    "    .mode(\"overwrite\")  # Overwrite if the file already exists\n",
    "    .option(\"header\", \"true\")  # Include header\n",
    "    .csv(final_dataset_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437856b-a4da-4e14-8856-9ae91ae572f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
